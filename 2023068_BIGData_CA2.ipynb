{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d5198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|  1|1467810672|        Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|                                                                                               is upset that he ...|\n",
      "|  2|1467810917|        Mon Apr 06 22:19:...|NO_QUERY|       mattycus|                                                                                               @Kenichan I dived...|\n",
      "|  3|1467811184|        Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|                                                                                               my whole body fee...|\n",
      "|  4|1467811193|        Mon Apr 06 22:19:...|NO_QUERY|         Karoli|                                                                                               @nationwideclass ...|\n",
      "|  5|1467811372|        Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|                                                                                               @Kwesidei not the...|\n",
      "|  6|1467811592|        Mon Apr 06 22:20:...|NO_QUERY|        mybirch|                                                                                                        Need a hug |\n",
      "|  7|1467811594|        Mon Apr 06 22:20:...|NO_QUERY|           coZZ|                                                                                               @LOLTrish hey  lo...|\n",
      "|  8|1467811795|        Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|                                                                                               @Tatiana_K nope t...|\n",
      "|  9|1467812025|        Mon Apr 06 22:20:...|NO_QUERY|        mimismo|                                                                                               @twittera que me ...|\n",
      "| 10|1467812416|        Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|                                                                                               spring break in p...|\n",
      "| 11|1467812579|        Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|                                                                                               I just re-pierced...|\n",
      "| 12|1467812723|        Mon Apr 06 22:20:...|NO_QUERY|           TLeC|                                                                                               @caregiving I cou...|\n",
      "| 13|1467812771|        Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|                                                                                               @octolinz16 It it...|\n",
      "| 14|1467812784|        Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|                                                                                               @smarrison i woul...|\n",
      "| 15|1467812799|        Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|                                                                                               @iamjazzyfizzle I...|\n",
      "| 16|1467812964|        Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|                                                                                               Hollis' death sce...|\n",
      "| 17|1467813137|        Mon Apr 06 22:20:...|NO_QUERY|       armotley|                                                                                               about to file taxes |\n",
      "| 18|1467813579|        Mon Apr 06 22:20:...|NO_QUERY|     starkissed|                                                                                               @LettyA ahh ive a...|\n",
      "| 19|1467813782|        Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|                                                                                               @FakerPattyPattz ...|\n",
      "| 20|1467813985|        Mon Apr 06 22:20:...|NO_QUERY|         quanvu|                                                                                               @alydesigns i was...|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/user/hduser/project_tweets/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4721cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 1599999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled dataset size: 1647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.appName(\"sample_tweets\").getOrCreate()\n",
    "\n",
    "# CSV file from HDFS into a PySpark DataFrame\n",
    "df = spark.read.csv(\"hdfs://localhost:9000/user/hduser/project_tweets/ProjectTweets.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# the number of rows in the original dataset\n",
    "print(\"Original dataset size:\", df.count())\n",
    "\n",
    "# Sample 1/1000th of the data\n",
    "sampled_df = df.sample(withReplacement=False, fraction=0.001, seed=42)\n",
    "\n",
    "#the number of rows in the sampled dataset\n",
    "print(\"Sampled dataset size:\", sampled_df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96cc809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in '0': 0\n",
      "Number of missing values in '1467810369': 0\n",
      "Number of missing values in 'Mon Apr 06 22:19:45 PDT 2009': 0\n",
      "Number of missing values in 'NO_QUERY': 0\n",
      "Number of missing values in '_TheSpecialOne_': 0\n",
      "Number of missing values in '@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Checking for missing values in the entire DataFrame\n",
    "missing_counts = [df.filter(col(\"`{}`\".format(c)).isNull()).count() for c in df.columns]\n",
    "\n",
    "# Counting of missing values for each column\n",
    "for col_name, missing_count in zip(df.columns, missing_counts):\n",
    "    print(f\"Number of missing values in '{col_name}': {missing_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7ea140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|  1|1467810672|        Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|                                                                                               is upset that he ...|\n",
      "|  2|1467810917|        Mon Apr 06 22:19:...|NO_QUERY|       mattycus|                                                                                               @Kenichan I dived...|\n",
      "|  3|1467811184|        Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|                                                                                               my whole body fee...|\n",
      "|  4|1467811193|        Mon Apr 06 22:19:...|NO_QUERY|         Karoli|                                                                                               @nationwideclass ...|\n",
      "|  5|1467811372|        Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|                                                                                               @Kwesidei not the...|\n",
      "|  6|1467811592|        Mon Apr 06 22:20:...|NO_QUERY|        mybirch|                                                                                                        Need a hug |\n",
      "|  7|1467811594|        Mon Apr 06 22:20:...|NO_QUERY|           coZZ|                                                                                               @LOLTrish hey  lo...|\n",
      "|  8|1467811795|        Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|                                                                                               @Tatiana_K nope t...|\n",
      "|  9|1467812025|        Mon Apr 06 22:20:...|NO_QUERY|        mimismo|                                                                                               @twittera que me ...|\n",
      "| 10|1467812416|        Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|                                                                                               spring break in p...|\n",
      "| 11|1467812579|        Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|                                                                                               I just re-pierced...|\n",
      "| 12|1467812723|        Mon Apr 06 22:20:...|NO_QUERY|           TLeC|                                                                                               @caregiving I cou...|\n",
      "| 13|1467812771|        Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|                                                                                               @octolinz16 It it...|\n",
      "| 14|1467812784|        Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|                                                                                               @smarrison i woul...|\n",
      "| 15|1467812799|        Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|                                                                                               @iamjazzyfizzle I...|\n",
      "| 16|1467812964|        Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|                                                                                               Hollis' death sce...|\n",
      "| 17|1467813137|        Mon Apr 06 22:20:...|NO_QUERY|       armotley|                                                                                               about to file taxes |\n",
      "| 18|1467813579|        Mon Apr 06 22:20:...|NO_QUERY|     starkissed|                                                                                               @LettyA ahh ive a...|\n",
      "| 19|1467813782|        Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|                                                                                               @FakerPattyPattz ...|\n",
      "| 20|1467813985|        Mon Apr 06 22:20:...|NO_QUERY|         quanvu|                                                                                               @alydesigns i was...|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274ec882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+----------------------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|summary|                 0|          1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|     _TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "+-------+------------------+--------------------+----------------------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|  count|           1599999|             1599999|                     1599999| 1599999|             1599999|                                                                                                            1599999|\n",
      "|   mean|          800000.0|1.9988178841753244E9|                        null|    null| 4.325887521835714E9|                                                                                                               null|\n",
      "| stddev|461880.07101411076|1.9357567891728687E8|                        null|    null|5.162733218454889E10|                                                                                                               null|\n",
      "|    min|                 1|          1467810672|        Fri Apr 17 20:30:...|NO_QUERY|        000catnap000|                                                                                                                ...|\n",
      "|    max|           1599999|          2329205794|        Wed May 27 07:27:...|NO_QUERY|          zzzzeus111|                                                                                               ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+----------------------------+--------+--------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0e08bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '1467810369',\n",
       " 'Mon Apr 06 22:19:45 PDT 2009',\n",
       " 'NO_QUERY',\n",
       " '_TheSpecialOne_',\n",
       " \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The column names in the DataFrame\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0b4588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:31:35,600 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 2.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         word_vector|\n",
      "+--------------------+\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|(262144,[171547],...|\n",
      "|      (262144,[],[])|\n",
      "|(262144,[58838],[...|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|(262144,[213714],...|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|      (262144,[],[])|\n",
      "|(262144,[156260],...|\n",
      "|      (262144,[],[])|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:45 PDT 2009|NO_QUERY|_TheSpecialOne_|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|text_length|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|  1|1467810672|        Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|                                                                                               is upset that he ...|          1|\n",
      "|  2|1467810917|        Mon Apr 06 22:19:...|NO_QUERY|       mattycus|                                                                                               @Kenichan I dived...|          1|\n",
      "|  3|1467811184|        Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|                                                                                               my whole body fee...|          1|\n",
      "|  4|1467811193|        Mon Apr 06 22:19:...|NO_QUERY|         Karoli|                                                                                               @nationwideclass ...|          1|\n",
      "|  5|1467811372|        Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|                                                                                               @Kwesidei not the...|          1|\n",
      "|  6|1467811592|        Mon Apr 06 22:20:...|NO_QUERY|        mybirch|                                                                                                        Need a hug |          1|\n",
      "|  7|1467811594|        Mon Apr 06 22:20:...|NO_QUERY|           coZZ|                                                                                               @LOLTrish hey  lo...|          1|\n",
      "|  8|1467811795|        Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|                                                                                               @Tatiana_K nope t...|          1|\n",
      "|  9|1467812025|        Mon Apr 06 22:20:...|NO_QUERY|        mimismo|                                                                                               @twittera que me ...|          1|\n",
      "| 10|1467812416|        Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|                                                                                               spring break in p...|          2|\n",
      "| 11|1467812579|        Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|                                                                                               I just re-pierced...|          2|\n",
      "| 12|1467812723|        Mon Apr 06 22:20:...|NO_QUERY|           TLeC|                                                                                               @caregiving I cou...|          2|\n",
      "| 13|1467812771|        Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|                                                                                               @octolinz16 It it...|          2|\n",
      "| 14|1467812784|        Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|                                                                                               @smarrison i woul...|          2|\n",
      "| 15|1467812799|        Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|                                                                                               @iamjazzyfizzle I...|          2|\n",
      "| 16|1467812964|        Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|                                                                                               Hollis' death sce...|          2|\n",
      "| 17|1467813137|        Mon Apr 06 22:20:...|NO_QUERY|       armotley|                                                                                               about to file taxes |          2|\n",
      "| 18|1467813579|        Mon Apr 06 22:20:...|NO_QUERY|     starkissed|                                                                                               @LettyA ahh ive a...|          2|\n",
      "| 19|1467813782|        Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|                                                                                               @FakerPattyPattz ...|          2|\n",
      "| 20|1467813985|        Mon Apr 06 22:20:...|NO_QUERY|         quanvu|                                                                                               @alydesigns i was...|          2|\n",
      "+---+----------+----------------------------+--------+---------------+-------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import length, col\n",
    "\n",
    "# Convert column '0' to string type\n",
    "df = df.withColumn('0', col('0').cast('string'))\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(inputCol='0', outputCol='words')\n",
    "df_words = tokenizer.transform(df)\n",
    "\n",
    "# Counting vectorize the words\n",
    "cv = CountVectorizer(inputCol='words', outputCol='word_vector')\n",
    "cv_model = cv.fit(df_words)\n",
    "df_word_vector = cv_model.transform(df_words)\n",
    "\n",
    "# The most common words\n",
    "df_word_vector.select('word_vector').show()\n",
    "\n",
    "# Calculating the length of each tweet\n",
    "df_with_length = df.withColumn('text_length', length(col('0')))\n",
    "df_with_length.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664840ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---------------+\n",
      "|  0|text_length|sentiment_score|\n",
      "+---+-----------+---------------+\n",
      "|  1|          1|        neutral|\n",
      "|  2|          1|        neutral|\n",
      "|  3|          1|        neutral|\n",
      "|  4|          1|        neutral|\n",
      "|  5|          1|        neutral|\n",
      "|  6|          1|        neutral|\n",
      "|  7|          1|        neutral|\n",
      "|  8|          1|        neutral|\n",
      "|  9|          1|        neutral|\n",
      "| 10|          2|        neutral|\n",
      "| 11|          2|        neutral|\n",
      "| 12|          2|        neutral|\n",
      "| 13|          2|        neutral|\n",
      "| 14|          2|        neutral|\n",
      "| 15|          2|        neutral|\n",
      "| 16|          2|        neutral|\n",
      "| 17|          2|        neutral|\n",
      "| 18|          2|        neutral|\n",
      "| 19|          2|        neutral|\n",
      "| 20|          2|        neutral|\n",
      "+---+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function for sentiment analysis using VADER\n",
    "def analyze_sentiment(text):\n",
    "    # Use VADER to get the compound sentiment score\n",
    "    sentiment_score = sid.polarity_scores(text)['compound']\n",
    "    \n",
    "    # Classify the sentiment based on the compound score\n",
    "    if sentiment_score >= 0.05:\n",
    "        return 'positive'\n",
    "    elif sentiment_score <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Create a UDF (User Defined Function) for the sentiment analysis\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# Apply the UDF to the DataFrame to get sentiment scores\n",
    "df_with_sentiment = df_with_length.withColumn('sentiment_score', analyze_sentiment_udf('0'))\n",
    "\n",
    "# Display the DataFrame with sentiment scores\n",
    "df_with_sentiment.select('0', 'text_length', 'sentiment_score').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdd60414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:============================================>           (59 + 1) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|sentiment_score|  count|\n",
      "+---------------+-------+\n",
      "|       positive|      3|\n",
      "|        neutral|1599993|\n",
      "|       negative|      3|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 58:==================================================>     (68 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_sentiment_distribution = df_with_sentiment.groupBy('sentiment_score').count()\n",
    "df_sentiment_distribution.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9718edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Converting Spark DataFrame to Pandas DataFrame for visualization\n",
    "pd_sentiment_distribution = df_sentiment_distribution.toPandas()\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "sns.barplot(x='sentiment_score', y='count', data=pd_sentiment_distribution)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7ffc0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   0|\n",
      "+----+\n",
      "| 143|\n",
      "|1337|\n",
      "|1432|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "| 86|\n",
      "|182|\n",
      "|187|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 65:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# examples of texts with positive sentiment\n",
    "df_with_sentiment.filter(col('sentiment_score') == 'positive').select('0').show()\n",
    "\n",
    "# examples of texts with negative sentiment\n",
    "df_with_sentiment.filter(col('sentiment_score') == 'negative').select('0').show()\n",
    "\n",
    "# examples of texts with neutral sentiment\n",
    "df_with_sentiment.filter(col('sentiment_score') == 'neutral').select('0').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f06b569c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfaf2004",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "word_vector does not exist. Available: 0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, text_length, sentiment_score, label",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/3356678189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Trained the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Making predictions on the testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: word_vector does not exist. Available: 0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, text_length, sentiment_score, label"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Your existing code...\n",
    "\n",
    "df_with_sentiment = df_with_sentiment.withColumn(\n",
    "    'label',\n",
    "    when(col('sentiment_score') == 'positive', 2)\n",
    "    .when(col('sentiment_score') == 'neutral', 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Splitted the data into training and testing sets\n",
    "(training_data, testing_data) = df_with_sentiment.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Defined features (assuming 'word_vector' is the column with vectorized words)\n",
    "assembler = VectorAssembler(inputCols=['word_vector'], outputCol='features')\n",
    "\n",
    "# Created a Logistic Regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, labelCol='label')\n",
    "\n",
    "# Created a pipeline to assemble features and train the model\n",
    "pipeline = Pipeline(stages=[assembler, lr])\n",
    "\n",
    "# Trained the model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Making predictions on the testing data\n",
    "predictions = model.transform(testing_data)\n",
    "\n",
    "# Evaluated the model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol='label', predictionCol='prediction', metricName='accuracy'\n",
    ")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy of the sentiment analysis model: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c69c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining features\n",
    "assembler = VectorAssembler(inputCols=['vectorized_words'], outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db932c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1467810369: long (nullable = true)\n",
      " |-- Mon Apr 06 22:19:45 PDT 2009: string (nullable = true)\n",
      " |-- NO_QUERY: string (nullable = true)\n",
      " |-- _TheSpecialOne_: string (nullable = true)\n",
      " |-- @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string (nullable = true)\n",
      " |-- text_length: integer (nullable = true)\n",
      " |-- sentiment_score: string (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_sentiment.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a38aacb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`sentiment_score`' given input columns: [0, 1467810369, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_];\n'Project [0#725, 1467810369#126L, Mon Apr 06 22:19:45 PDT 2009#127, NO_QUERY#128, _TheSpecialOne_#129, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130, CASE WHEN ('sentiment_score = positive) THEN 2 WHEN ('sentiment_score = neutral) THEN 1 ELSE 0 END AS label#934]\n+- Project [cast(0#125 as string) AS 0#725, 1467810369#126L, Mon Apr 06 22:19:45 PDT 2009#127, NO_QUERY#128, _TheSpecialOne_#129, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130]\n   +- Relation[0#125,1467810369#126L,Mon Apr 06 22:19:45 PDT 2009#127,NO_QUERY#128,_TheSpecialOne_#129,@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/1343939982.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m df = df.withColumn(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \"\"\"\n\u001b[1;32m   2454\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`sentiment_score`' given input columns: [0, 1467810369, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_];\n'Project [0#725, 1467810369#126L, Mon Apr 06 22:19:45 PDT 2009#127, NO_QUERY#128, _TheSpecialOne_#129, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130, CASE WHEN ('sentiment_score = positive) THEN 2 WHEN ('sentiment_score = neutral) THEN 1 ELSE 0 END AS label#934]\n+- Project [cast(0#125 as string) AS 0#725, 1467810369#126L, Mon Apr 06 22:19:45 PDT 2009#127, NO_QUERY#128, _TheSpecialOne_#129, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130]\n   +- Relation[0#125,1467810369#126L,Mon Apr 06 22:19:45 PDT 2009#127,NO_QUERY#128,_TheSpecialOne_#129,@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D#130] csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "text_column = df.columns[-1]\n",
    "\n",
    "df = df.withColumn(\n",
    "    'label',\n",
    "    when(col('sentiment_score') == 'positive', 2)\n",
    "    .when(col('sentiment_score') == 'neutral', 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(inputCol=text_column, outputCol='words')\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "# Counting Vectorization\n",
    "vectorizer = CountVectorizer(inputCol='words', outputCol='text_vector')\n",
    "vector_model = vectorizer.fit(df)\n",
    "df = vector_model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d41d9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: string (nullable = true)\n",
      " |-- 1467810369: long (nullable = true)\n",
      " |-- Mon Apr 06 22:19:45 PDT 2009: string (nullable = true)\n",
      " |-- NO_QUERY: string (nullable = true)\n",
      " |-- _TheSpecialOne_: string (nullable = true)\n",
      " |-- @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40f8974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_with_sentiment = df_with_length.withColumn('sentiment_score', analyze_sentiment_udf(col('0')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "640bd4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df_with_sentiment.withColumn(\n",
    "    'label',\n",
    "    when(col('sentiment_score') == 'positive', 2)\n",
    "    .when(col('sentiment_score') == 'neutral', 1)\n",
    "    .otherwise(0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e470c5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dee32d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Output column words already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/3556244042.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Tokenizing the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Vectorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Output column words already exists."
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, VectorAssembler\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "\n",
    "df = df.withColumn(\n",
    "    'label',\n",
    "    when(col('0') == 'positive', 2)\n",
    "    .when(col('0') == 'neutral', 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Tokenizing the text\n",
    "tokenizer = Tokenizer(inputCol='0', outputCol='words')\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "#Vectorization\n",
    "vectorizer = CountVectorizer(inputCol='words', outputCol='text_vector')\n",
    "vector_model = vectorizer.fit(df)\n",
    "df = vector_model.transform(df)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=['text_vector'], outputCol='features')\n",
    "df = assembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a85b94cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: 0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, text_length, sentiment_score, label",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/1342329687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 'features' is the column with vectorized words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0massembler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature_vector'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 'train_df' contains a new column 'feature_vector' with the assembled features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: 0, 1467810369, Mon Apr 06 22:19:45 PDT 2009, NO_QUERY, _TheSpecialOne_, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D, text_length, sentiment_score, label"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 'features' is the column with vectorized words\n",
    "assembler = VectorAssembler(inputCols=['features'], outputCol='feature_vector')\n",
    "train_df = assembler.transform(training_data)\n",
    "\n",
    "# 'train_df' contains a new column 'feature_vector' with the assembled features\n",
    "\n",
    "# Defining and fitting the Logistic Regression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"feature_vector\", maxIter=10)\n",
    "model = lr.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8df0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c387419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 17:46:33,502 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 17.8 MiB\n",
      "2023-11-22 17:46:57,587 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 17.8 MiB\n",
      "2023-11-22 17:47:12,294 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 17.8 MiB\n",
      "2023-11-22 17:47:32,896 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 20.4 MiB\n",
      "2023-11-22 17:49:40,037 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1034.3 KiB\n",
      "2023-11-22 17:49:41,617 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 22.4 MiB\n",
      "2023-11-22 17:50:03,562 WARN memory.MemoryStore: Not enough space to cache rdd_258_0 in memory! (computed 272.9 MiB so far)\n",
      "2023-11-22 17:50:03,581 WARN storage.BlockManager: Persisting block rdd_258_0 to disk instead.\n",
      "[Stage 76:>                                                         (0 + 1) / 2]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/2321251414.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumTrees\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=30)\n",
    "model = rf.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7775f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentApp\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1c7fc5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date_format' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13282/1430268172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_time_aggregated_week\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_with_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_with_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yyyy-MM-dd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'week'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'avg'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_format' is not defined"
     ]
    }
   ],
   "source": [
    "df_time_aggregated_week = df_with_sentiment.groupBy(date_format(df_with_sentiment.columns[2], 'yyyy-MM-dd').alias('week')).agg({'sentiment_score': 'avg'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b752adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[0: string, 1467810369: bigint, Mon Apr 06 22:19:45 PDT 2009: string, NO_QUERY: string, _TheSpecialOne_: string, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string, text_length: int, sentiment_score: string]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_sentiment.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9322a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, quarter\n",
    "\n",
    "df_time_aggregated_week = df_with_sentiment.groupBy(date_format(df_with_sentiment.columns[2], 'yyyy-MM-dd').alias('week')).agg({'sentiment_score': 'mean'})\n",
    "\n",
    "df_time_aggregated_month = df_with_sentiment.groupBy(date_format(df_with_sentiment.columns[2], 'yyyy-MM').alias('month')).agg({'sentiment_score': 'mean'})\n",
    "\n",
    "df_time_aggregated_3months = df_with_sentiment.groupBy(quarter(df_with_sentiment.columns[2]).alias('quarter')).agg({'sentiment_score': 'mean'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b8d0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 18:17:16,862 WARN execution.CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[0: string, 1467810369: bigint, Mon Apr 06 22:19:45 PDT 2009: string, NO_QUERY: string, _TheSpecialOne_: string, @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D: string, text_length: int, sentiment_score: string]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_sentiment.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61832ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_aggregated_week.show()\n",
    "df_time_aggregated_month.show()\n",
    "df_time_aggregated_3months.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9596a650",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2789471359.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_14821/2789471359.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed8645ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14821/3999437562.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .parameter import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mUninitializedBuffer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIdentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBilinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mConvTranspose1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConvTranspose3d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mLazyConv1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyConv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyConv3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyConvTranspose1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyConvTranspose2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyConvTranspose3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parameter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_disabled_torch_function_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Extracting sentiment scores and scaling the data\n",
    "sentiment_data = df_time_aggregated_week.select('sentiment_score').toPandas()\n",
    "scaler = MinMaxScaler()\n",
    "sentiment_data_scaled = scaler.fit_transform(sentiment_data)\n",
    "\n",
    "# Converting the data into sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        label = data[i+seq_length:i+seq_length+1]\n",
    "        sequences.append((seq, label))\n",
    "    return sequences\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 5\n",
    "input_size = 1\n",
    "hidden_size = 100\n",
    "num_layers = 1\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "\n",
    "# Preparing data\n",
    "sequences = create_sequences(sentiment_data_scaled, sequence_length)\n",
    "X = np.array([item[0] for item in sequences])\n",
    "y = np.array([item[1] for item in sequences])\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Converting to PyTorch tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test)\n",
    "\n",
    "#LSTM model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = LSTM(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    mse = mean_squared_error(y_test, test_outputs)\n",
    "    print(f'Mean Squared Error on Test Data: {mse:.4f}')\n",
    "\n",
    "# the predictions and true labels to original scale\n",
    "test_outputs_original = scaler.inverse_transform(test_outputs.numpy())\n",
    "y_test_original = scaler.inverse_transform(y_test.numpy())\n",
    "\n",
    "# Plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(test_outputs_original, label='Predicted')\n",
    "plt.plot(y_test_original, label='True')\n",
    "plt.title('LSTM Time Series Forecasting')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaed0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
